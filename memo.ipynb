{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEMO Assignment\n",
    "Andrea De Carlo and Pooya Torabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# todo need to move installation to requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T20:36:43.376504Z",
     "start_time": "2024-05-08T20:36:40.453540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# clip-ViT-B-16, https://huggingface.co/openai/clip-vit-base-patch16\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "pretrained_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9916e-01, 8.4369e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = pretrained_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "\n",
    "print(probs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T20:36:46.934960Z",
     "start_time": "2024-05-08T20:36:45.146739Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "CLIPOutput(loss=None, logits_per_image=tensor([[22.7387, 15.6618]], grad_fn=<TBackward0>), logits_per_text=tensor([[22.7387],\n        [15.6618]], grad_fn=<MulBackward0>), text_embeds=tensor([[ 0.0413, -0.0037,  0.0096,  ...,  0.0059, -0.0040,  0.0131],\n        [ 0.0282, -0.0117,  0.0112,  ..., -0.0110,  0.0240,  0.0283]],\n       grad_fn=<DivBackward0>), image_embeds=tensor([[ 4.4385e-02, -4.7227e-02, -1.8999e-02, -5.5320e-02, -2.4445e-02,\n         -2.7468e-02,  8.0666e-03,  5.5988e-02,  6.6346e-03, -4.3237e-02,\n          6.8330e-03,  5.3842e-02,  1.9102e-02, -1.9645e-02, -1.9089e-02,\n         -3.6711e-02, -6.4628e-03,  3.6136e-02,  4.8417e-02, -1.2489e-02,\n          1.5586e-02, -3.4931e-02, -3.0641e-02, -4.9630e-02,  3.2662e-02,\n         -5.4957e-02,  3.6367e-03,  7.5559e-03,  1.0419e-02,  2.0192e-02,\n         -4.6395e-03, -8.5798e-03,  4.1773e-02, -1.9621e-02, -7.7700e-02,\n         -1.1550e-03,  7.6500e-02,  4.1755e-03,  1.8130e-02,  7.9161e-03,\n          2.0843e-02, -6.0526e-03,  4.3204e-02,  3.5115e-02, -1.2037e-02,\n         -2.7802e-02,  1.0441e-02,  6.0584e-02, -3.6130e-02, -1.3163e-03,\n         -1.5618e-01, -6.0172e-03,  1.8057e-02,  7.7999e-03, -1.2349e-02,\n         -7.0975e-02, -4.1785e-02, -3.0035e-03, -1.7642e-02, -4.7434e-04,\n          8.5521e-02, -1.4926e-02,  5.0824e-03, -6.2995e-03,  2.0203e-02,\n          1.7694e-03,  8.6718e-03, -1.5835e-02,  2.2321e-02, -2.1615e-02,\n          1.6842e-02, -2.3203e-02, -2.5554e-02, -2.5878e-02, -4.0010e-02,\n         -9.9506e-04, -3.0015e-02, -1.2315e-02,  3.5569e-02, -2.1028e-02,\n          2.9953e-03, -5.4381e-02, -8.1291e-02,  1.8695e-03,  2.0071e-02,\n          1.3275e-02,  2.1480e-02,  2.3406e-02, -4.1696e-02, -2.4905e-02,\n         -2.8401e-03, -1.2919e-02, -6.9735e-03,  6.4305e-03, -1.4692e-02,\n          1.3441e-02, -7.5204e-03,  3.1159e-02, -1.0579e-02, -6.7892e-03,\n         -8.4715e-04, -1.2355e-02,  9.4608e-03,  2.2561e-02, -1.7969e-02,\n         -1.4797e-02,  4.5861e-03,  3.3810e-02,  3.8498e-02, -4.9484e-02,\n         -2.8427e-02, -3.7108e-02,  1.2693e-02, -5.0613e-03,  3.5905e-02,\n         -4.3416e-02, -2.3375e-02, -4.2046e-02,  4.7201e-02,  2.7410e-02,\n          2.7381e-03, -5.8447e-02, -6.6693e-03, -1.1198e-02,  4.4565e-02,\n          2.3828e-02,  8.1889e-02, -1.1111e-04,  5.3514e-02, -5.5029e-03,\n         -1.5770e-02, -2.0796e-02,  4.4663e-02,  3.0609e-02,  4.5812e-02,\n         -7.7195e-03,  2.9523e-02, -4.7352e-02, -5.0286e-02,  3.7578e-02,\n          3.0505e-02, -5.1269e-02, -1.6372e-02, -9.8038e-03,  2.1039e-02,\n          2.6195e-02,  3.9717e-02, -4.3397e-02, -9.0217e-03,  2.5035e-02,\n         -1.2807e-02, -2.3548e-02, -5.8736e-03, -9.1279e-03,  3.8544e-02,\n         -8.5456e-03, -4.4053e-03,  8.3884e-02,  3.3422e-02, -1.6013e-02,\n         -2.4417e-02,  6.1244e-02, -4.3839e-02,  4.9303e-02, -6.7361e-03,\n         -3.9732e-02,  4.4559e-02,  6.2046e-03, -2.2773e-02, -2.1174e-02,\n         -4.7904e-02,  3.1555e-02, -3.3759e-02, -2.1465e-02,  1.0972e-02,\n          7.4479e-03, -2.4411e-02,  1.2653e-02,  1.7947e-02,  9.5519e-03,\n          3.2834e-03,  1.4729e-02,  1.3326e-02, -1.5261e-02,  1.5368e-02,\n          6.1585e-03,  1.3483e-02, -2.7494e-02, -5.1603e-02,  3.7135e-04,\n          3.6750e-02,  1.7355e-03, -9.1555e-03,  6.9050e-02,  2.2546e-02,\n          4.4658e-02,  4.3741e-02,  2.3905e-02,  4.9938e-04, -2.6188e-04,\n         -5.0739e-02,  4.4602e-03,  1.5014e-02,  8.6108e-02,  2.2323e-02,\n         -3.4667e-03,  8.1340e-03, -2.9136e-02,  5.6983e-02, -5.6231e-03,\n         -8.9332e-03,  2.9187e-02, -2.7645e-03, -9.0153e-03,  2.7829e-02,\n         -6.6043e-03,  5.2817e-03, -2.0190e-02, -3.7715e-03, -7.6434e-02,\n         -2.6692e-02,  4.4222e-03,  2.2172e-02, -3.1993e-02, -1.9620e-02,\n          6.5972e-03, -8.6038e-03,  2.1600e-02,  4.7580e-03,  1.4031e-02,\n         -2.1299e-02,  9.7276e-03, -1.6724e-02,  5.3539e-03, -4.7970e-03,\n         -1.6392e-02,  9.9766e-04,  1.6421e-02,  1.9549e-02,  1.8841e-03,\n          1.8529e-02, -9.0191e-03, -3.2900e-02,  8.2999e-03,  3.3114e-02,\n         -2.6101e-02, -2.3850e-02,  2.9919e-02, -9.5394e-03, -6.9578e-02,\n          4.2154e-02, -5.2922e-02,  4.5003e-02, -1.9527e-02,  1.6428e-03,\n         -3.8254e-02,  5.4488e-04,  4.8873e-02,  3.6890e-02,  2.9539e-02,\n         -1.0990e-02,  7.7667e-03,  5.5696e-02, -3.0565e-02,  1.1519e-02,\n         -9.7197e-02, -4.3621e-02, -1.5812e-02,  1.7427e-02, -4.3138e-02,\n          2.2940e-02,  4.6542e-02, -2.9095e-02, -3.9852e-02,  1.3376e-02,\n         -4.5936e-02, -3.0104e-02, -2.0282e-02,  1.7199e-02, -4.8699e-02,\n         -1.3466e-01, -3.9680e-02, -7.5597e-03,  5.3965e-02, -3.4796e-03,\n          3.5174e-02, -6.2683e-03, -2.6964e-02, -2.3131e-02, -1.0723e-02,\n         -3.2454e-02,  2.5237e-02, -6.5696e-02,  3.1530e-02,  2.3310e-02,\n         -2.2714e-02, -6.3271e-03,  5.4561e-02, -5.2755e-02, -1.4637e-02,\n         -3.0736e-04, -8.2146e-04, -2.6069e-02, -7.5432e-03, -7.1884e-04,\n          3.5934e-02,  1.3903e-02, -3.5526e-02, -9.6063e-03, -4.1010e-02,\n          4.4496e-02,  2.5907e-02,  6.4210e-02, -1.8314e-02,  5.6840e-02,\n          1.3771e-02, -6.7960e-03,  3.1536e-02,  3.8088e-02, -3.8346e-02,\n         -9.1556e-03,  1.3203e-02,  1.5936e-02,  1.1033e-02,  5.8192e-03,\n          3.2491e-02,  3.6617e-02,  7.3801e-04, -2.4035e-02,  2.3443e-02,\n         -1.2366e-02, -1.3606e-02,  6.7057e-02,  3.3677e-02,  2.6203e-02,\n         -5.0269e-02,  1.9226e-02,  2.5475e-02,  1.8324e-02,  2.0043e-02,\n          2.6908e-02,  7.7088e-03,  9.0580e-02,  1.4760e-02,  8.9082e-02,\n          3.4568e-02, -2.1491e-02, -5.3523e-03,  3.8149e-02,  1.2928e-02,\n          1.1713e-01,  4.8946e-02,  9.3616e-03,  1.5489e-02,  3.9201e-02,\n          2.9725e-02,  1.8606e-02, -1.8245e-02, -1.9288e-02,  1.4420e-02,\n          9.9382e-03, -1.1775e-02,  1.0054e-01,  6.6736e-02, -4.2379e-03,\n          4.4959e-02,  9.5374e-03, -2.0058e-02,  5.3936e-02,  2.5555e-02,\n         -2.7358e-02, -7.3449e-03, -3.8593e-02,  2.2918e-03,  1.0292e-01,\n          3.5763e-02,  7.1944e-02,  4.4674e-02, -5.5842e-02,  4.5454e-02,\n         -1.7010e-03, -1.6208e-02, -4.4273e-02,  4.0558e-02, -3.2191e-02,\n         -4.3011e-03,  3.3515e-02,  2.4088e-02, -2.9729e-02,  3.4871e-02,\n          1.2745e-02,  1.7297e-02,  2.2522e-02,  2.3758e-02,  1.4220e-02,\n          1.3370e-02,  5.4749e-04,  4.4878e-03,  3.6928e-02,  2.8961e-02,\n         -4.7170e-02, -2.9496e-02, -3.0447e-02,  1.2377e-03,  3.3045e-02,\n          1.5245e-02,  9.6993e-02,  1.2364e-02, -2.2507e-02, -2.5839e-03,\n          2.6801e-02, -2.5848e-02, -8.3150e-02, -6.4949e-02,  4.1854e-02,\n          7.2848e-03,  4.5755e-02, -2.5160e-02, -4.0189e-02,  7.4113e-03,\n          4.3443e-02, -1.5763e-02,  1.8963e-02, -9.4509e-03, -3.4138e-02,\n         -3.7278e-02,  5.1415e-02,  1.9248e-02,  1.6352e-02,  3.2428e-02,\n         -3.2550e-02, -1.3623e-02, -4.5675e-02,  3.5318e-02, -5.6609e-02,\n         -2.4318e-02,  4.9534e-02,  3.6829e-02, -1.3060e-03,  2.4467e-02,\n         -6.6014e-03, -4.7031e-02,  6.0930e-01,  2.5857e-02, -1.3680e-02,\n         -2.5587e-02,  4.7795e-02,  3.0177e-03, -4.8291e-02, -3.1578e-02,\n         -9.2472e-02, -3.0026e-02, -4.5867e-03, -4.2193e-03, -2.7476e-02,\n          2.2022e-03,  8.5782e-03,  1.0099e-02,  2.1050e-02, -1.1377e-02,\n         -1.0746e-02, -3.6273e-02, -3.2924e-02, -6.6734e-02,  7.1422e-03,\n         -1.5458e-01,  2.2258e-02, -1.3991e-03, -6.2940e-03,  2.2655e-02,\n          2.7524e-02, -2.1035e-03,  1.8240e-02,  1.4341e-02, -4.4829e-02,\n          1.1051e-01,  3.2413e-02, -4.1134e-03, -3.6591e-02, -1.5891e-02,\n          6.7484e-02,  1.7595e-02,  9.9863e-03, -8.0917e-03,  9.2348e-03,\n          2.8919e-02,  9.7572e-03,  2.1861e-02, -1.5439e-02, -2.1688e-02,\n          1.6180e-02,  3.8453e-02, -2.3430e-02,  5.6159e-02,  3.3900e-02,\n         -3.6627e-04,  1.5032e-02, -2.5057e-02, -1.1697e-03,  3.9255e-02,\n         -1.8698e-02, -3.6316e-02, -4.7452e-02,  3.5574e-05, -3.6564e-02,\n          4.5725e-02,  1.0827e-02,  6.9629e-03,  2.0202e-02,  2.3294e-02,\n         -1.1114e-03, -7.4397e-04]], grad_fn=<DivBackward0>), text_model_output=BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0134,  0.0697,  0.0683,  ..., -0.3548,  0.1088, -0.3185],\n         [ 0.1461, -0.2560,  1.6223,  ...,  0.5142,  0.3926, -0.7674],\n         [-0.5576, -0.5332,  0.5207,  ...,  0.7968,  0.2744, -0.9393],\n         ...,\n         [-0.7624, -0.5579,  1.4276,  ...,  0.2376, -0.6755, -1.2623],\n         [ 1.0121, -0.1068, -0.5859,  ..., -1.6663,  1.2639, -0.6731],\n         [ 0.0200,  0.0364,  0.5863,  ..., -0.7169,  0.2894, -0.3139]],\n\n        [[ 0.0134,  0.0697,  0.0683,  ..., -0.3548,  0.1088, -0.3185],\n         [ 0.1461, -0.2560,  1.6223,  ...,  0.5142,  0.3926, -0.7674],\n         [-0.5576, -0.5332,  0.5207,  ...,  0.7968,  0.2744, -0.9393],\n         ...,\n         [-0.7624, -0.5579,  1.4276,  ...,  0.2376, -0.6755, -1.2623],\n         [-1.2065, -1.1087, -0.6691,  ..., -1.8994,  0.8293, -0.4774],\n         [-1.2898, -1.0403,  0.5725,  ..., -0.5004,  0.2878, -0.2958]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.0200,  0.0364,  0.5863,  ..., -0.7169,  0.2894, -0.3139],\n        [-1.2898, -1.0403,  0.5725,  ..., -0.5004,  0.2878, -0.2958]],\n       grad_fn=<IndexBackward0>), hidden_states=None, attentions=None), vision_model_output=BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.1898,  0.0189, -0.1571,  ..., -0.1360,  0.2989, -0.1894],\n         [-0.2411,  0.3947,  0.8216,  ..., -0.3708,  0.4311, -0.3889],\n         [-0.4329,  0.3757,  0.3164,  ..., -0.5861,  0.5358, -0.7669],\n         ...,\n         [-0.4683,  0.7267,  0.7386,  ..., -0.2629,  0.4651, -0.8609],\n         [-0.1463,  1.1881,  0.8768,  ...,  0.0939,  0.5914, -0.7540],\n         [ 0.0837,  0.2681,  1.2285,  ..., -0.4405,  0.1398, -0.5319]]],\n       grad_fn=<AddBackward0>), pooler_output=tensor([[ 1.6633e-01, -1.6071e-01, -5.3910e-01, -2.2258e-02, -4.0967e-01,\n          5.2225e-01,  1.2000e+00, -4.1941e-01,  2.1200e-01, -8.3154e-01,\n          1.4934e+00, -8.0277e-01,  1.0096e+00, -3.7143e-01, -8.5565e-01,\n          4.1863e-02, -1.0155e+00, -3.1717e-01,  1.8272e-01,  6.6551e-01,\n         -3.3608e-02, -1.3333e+00, -6.5437e-01, -8.8493e-02,  3.8730e-01,\n          9.3928e-01,  3.7007e-01,  8.1352e-01,  5.4275e-01, -1.3873e-01,\n          2.6510e-01,  3.1816e-01,  3.5766e-02,  1.0973e+00,  1.4405e+00,\n          9.5517e-01, -1.4174e-01,  1.5112e+00, -1.0917e+00, -7.6769e-01,\n          3.8701e-01,  4.9362e-01,  1.1562e+00,  8.6274e-01, -1.3037e+00,\n          2.9003e-01,  2.4218e-01, -3.4188e-01, -3.6062e-01,  3.4322e-01,\n          5.8366e-02,  1.2524e+00,  1.2329e+00,  6.6338e-01,  3.4179e-02,\n          2.7881e-01,  1.4874e+00, -2.3350e+00,  3.5872e-01, -6.7503e-01,\n          7.1257e-01, -1.3602e-01, -1.6524e+00,  1.8244e+00,  1.1184e+00,\n          4.8609e+00, -8.6631e-01, -2.2309e-02,  6.3105e-01,  1.6198e-01,\n         -2.1573e+00,  2.6193e-02,  3.0283e-01, -7.9843e-02, -1.5733e-01,\n          1.2051e+00,  9.3950e-01, -3.9115e-01, -4.1135e-01, -3.9921e-01,\n          2.0882e+00,  1.7318e+00, -2.4224e-01,  7.5465e-02, -3.1969e-02,\n         -4.6818e-01, -1.0677e-01, -1.6458e-01,  7.3552e-01, -1.0476e-01,\n         -1.0133e+00,  5.0482e-01,  1.1088e+00, -2.8575e-01, -3.9373e-02,\n         -4.4637e-01,  1.2048e-01,  6.7862e-01,  4.0806e-02, -4.4760e-01,\n         -1.7821e-01,  6.7023e-01, -4.6183e-01,  1.1744e+00,  5.6208e-01,\n          1.4716e+00, -1.4462e-01, -1.3805e-01, -3.5748e-01, -1.4378e+00,\n          1.3071e+00, -4.1622e-01,  6.7809e-01, -1.0503e+00,  1.2778e+00,\n         -2.5502e-01,  1.1794e-01, -1.6311e+00,  4.4150e-01,  1.1986e-01,\n         -3.0270e-01, -2.2733e-02, -1.0739e+00,  1.3620e-03,  2.1312e-01,\n         -3.5550e-01, -3.3940e-02, -8.2556e-01, -2.8892e-01,  2.0218e-01,\n          4.3301e-01,  2.4787e-01,  1.0889e-01,  9.7549e-01, -8.5100e-01,\n          1.6149e+00, -1.7188e+00, -1.4219e+00,  7.5751e-02,  6.5467e-01,\n          9.8975e-01, -4.4907e-03,  2.5822e+00,  3.8948e-01,  1.6244e-01,\n         -3.4447e-01, -3.2291e-01, -1.3394e+00, -5.1744e-01,  5.4769e-02,\n          5.8356e-01,  1.2481e+00,  1.2550e+00,  1.5129e-01,  2.0048e-01,\n          1.4484e-01,  3.8341e-01,  1.1315e+00,  1.1891e+00, -1.1945e+00,\n          9.9856e-01,  1.0876e+00, -5.1677e-02, -1.2431e-01,  1.0364e+00,\n         -2.7171e-02,  9.2363e-01,  2.5863e-01,  1.7076e+00,  8.4359e-01,\n          1.1013e+00,  3.2709e+00,  7.8516e-04, -4.0161e-01,  1.1539e+00,\n          8.2489e-01,  1.3688e+00, -1.3556e+00, -4.4139e-01, -7.1854e-01,\n         -6.5270e-01, -1.0257e-01,  2.5601e-01,  7.3704e-01,  3.6949e-01,\n          3.9270e-01,  6.0408e-01, -1.4837e+00, -5.7870e-01,  7.3607e-01,\n          4.2468e-01, -4.3624e-01,  1.1142e+00, -9.6014e-01, -1.3181e-01,\n         -5.5390e-01, -1.1696e-01, -1.0142e+00,  1.0079e+00,  2.7566e-01,\n         -1.5218e+00,  5.5136e-01,  8.0463e-01,  4.5949e-01,  4.2333e-01,\n          1.9498e-01,  1.7450e+00,  1.0686e-01, -1.1669e-01,  4.3607e-01,\n          1.5551e+00,  7.9839e-01,  4.2259e-01,  1.7726e-02,  1.4936e-02,\n          4.0909e-01,  4.7938e-01,  2.1448e-01, -1.4211e-01,  1.0904e+00,\n          1.3266e-01,  1.0403e+00, -1.0878e-02,  7.4633e-01,  1.1481e-01,\n          9.9514e-01, -5.4276e-02, -1.3112e-01,  1.5069e+00,  8.8017e-01,\n          3.8107e-01, -3.2207e-01,  1.5531e+00,  3.2747e-01,  9.9211e-01,\n         -5.3525e-01,  1.0303e+00, -1.0982e+00,  8.4038e-01,  1.8891e+00,\n          2.6366e-01,  8.9665e-01, -8.1275e-01,  1.5550e+00, -4.1578e-01,\n         -1.6877e-01, -3.6244e-01,  7.2142e-01, -1.6307e-01,  4.8130e-01,\n         -2.9689e-01, -3.2946e-01,  1.0341e+00, -8.8719e-01,  5.4413e-01,\n         -2.4362e-01,  2.9706e-01,  1.3054e+00, -7.2526e-02,  3.1098e+00,\n          1.6872e+00,  8.8339e-01,  3.8205e-01,  6.4482e-01,  8.7936e-01,\n          6.2453e-01, -9.9369e-01, -1.9594e+00, -3.4999e-03,  4.3049e-01,\n         -1.2482e+00, -6.1004e-01,  6.9687e-01,  5.0077e-01,  7.1337e-01,\n          4.7439e-02, -5.6746e-01,  8.1000e-01, -6.6659e-01,  1.0419e+00,\n         -6.7926e-02,  5.6439e-01, -8.8846e-01, -1.3367e-01,  1.8000e+00,\n          4.3405e-01,  5.8168e-01,  4.8928e-01, -1.4642e-01, -7.6319e-01,\n          5.0189e-01, -1.3114e-01,  9.0009e-01, -1.1722e-01,  5.3603e-01,\n         -6.2759e-01,  1.3758e+00, -8.6016e-02,  5.5972e-01,  2.1859e+00,\n         -8.5936e-01, -4.8721e-02,  1.3338e+00,  5.7133e-01,  3.0330e-01,\n         -6.0644e-02, -8.0492e-01, -1.3845e+00,  7.9520e-01, -2.8791e+00,\n         -1.1404e+00,  1.4472e-01,  4.2876e-01, -6.7624e-04,  4.1573e-01,\n         -4.5602e-02,  7.7840e-01, -8.2294e-02, -6.0326e-01, -4.2043e-01,\n          4.8100e-01,  6.9113e-01, -7.2417e-01,  4.5340e-01,  6.4306e-01,\n         -4.4494e-01,  1.2194e+00,  1.8710e-01,  7.0242e-01, -7.7156e-01,\n          3.8289e-01,  4.5533e-01, -9.8588e-01, -3.2327e-01,  1.2400e+00,\n          1.8473e+00,  1.6162e-01, -5.0642e-01, -7.5633e-01,  6.8170e-01,\n         -9.1015e-01,  5.1707e-01, -1.4766e+00,  5.6429e-01,  2.0392e-01,\n          9.2984e-01,  1.8355e-01, -4.0141e-01,  7.8804e-02,  1.5584e+00,\n          2.4492e+00,  7.6389e-01,  3.2920e-01,  1.2560e+00,  2.5944e-01,\n         -3.3840e-01, -5.4225e-01, -2.7324e-01, -5.2298e-01,  3.1170e-02,\n          1.1265e-01, -5.9024e-01,  1.8836e+00,  1.6840e+00,  4.8223e-01,\n         -7.1469e-01, -4.3342e-01,  5.3235e-01,  2.0355e+00, -6.5493e-03,\n         -3.3159e-02, -6.2058e-01,  1.1699e+00, -3.0570e-03, -1.9253e-01,\n          1.0905e+00,  6.2627e-01, -1.0780e+00, -7.4710e-01, -3.0013e-01,\n         -4.7030e-01,  2.8956e-01, -1.2575e+00,  7.6045e-01,  6.0899e-01,\n         -8.8166e-01,  9.8515e-01, -7.7325e-01, -5.7292e-01,  4.1750e-01,\n         -1.3503e+00, -1.4836e+00,  8.1791e-01, -2.8081e+00,  2.0683e+00,\n          1.4181e+00,  2.3561e+00, -4.0921e-01,  5.8285e-02,  8.1723e-01,\n          1.2986e+00,  1.1012e-01,  5.9247e-01,  3.8883e-01, -2.3415e-01,\n          2.2876e-01, -1.7937e+00,  1.1988e+00,  4.7592e-01, -5.4970e-01,\n          6.6414e-01,  6.1364e-01, -5.6630e-01, -1.7249e+00,  9.5610e-01,\n          1.2646e+00, -1.2476e-01, -2.2937e+00,  6.7904e-02,  1.2742e+00,\n          1.9929e-01,  5.5213e-01,  9.4948e-03,  1.2246e+00, -4.7265e-01,\n         -2.9101e-01,  5.1432e-01,  9.7515e-01,  7.9307e-01,  3.4591e+00,\n          1.2780e+00,  6.7395e-01,  9.4301e-01, -3.9426e-01, -1.6892e-01,\n         -2.0754e+00,  7.1843e-02,  1.3624e-02,  2.5358e-01, -6.1242e-01,\n          2.1316e-01, -1.5784e-01,  7.6719e-01, -1.5330e+00,  5.5318e-01,\n         -1.6462e-01, -1.5536e-01,  8.7525e-01, -3.9022e-01,  9.2777e-01,\n          5.9322e-01,  6.3190e-01, -6.7796e-01,  1.6417e+00,  2.0220e-02,\n          3.9294e-01,  3.3679e-01,  9.8842e-02, -7.4342e-02,  5.4890e-01,\n          1.1153e-01,  2.5610e+00, -9.0258e-01, -3.1166e-01, -7.2254e-01,\n          1.3948e-01,  6.8989e-01,  1.1416e+00, -5.2158e-01, -3.2870e-01,\n         -1.1495e+00,  1.8267e+00, -3.6279e-01,  7.9648e-02, -1.7793e+00,\n          7.5851e-02,  4.6005e-01,  4.5486e-01,  9.5753e-01,  1.3430e+00,\n         -3.3289e-01, -6.3001e-01,  1.3959e+00,  1.0486e+00,  1.0010e+00,\n         -8.5429e-01, -6.8480e-01, -6.2984e-01, -4.9557e-01, -1.0275e+00,\n         -8.3113e-01, -6.8724e-01,  1.5066e+00,  2.7415e-01, -6.0938e-01,\n          1.0049e+00, -7.4589e-01,  2.3000e-01, -8.1337e-01,  1.2103e+00,\n          9.5388e-01,  6.8443e-01, -2.2316e-01,  4.9620e-01, -6.9226e-01,\n          1.8199e+00,  1.4225e-01,  4.4600e-01,  1.4680e+00,  1.2363e+00,\n         -2.1070e+00, -2.2186e+00,  6.0716e-01,  2.6732e-01, -5.5477e-01,\n          1.4827e-01,  5.8782e-01,  6.2268e-01,  1.9212e-01,  2.2740e+00,\n          4.8726e-01,  1.9966e+00,  3.7817e-01,  1.0750e+00,  5.8582e-01,\n          8.2391e-01,  8.1459e-02,  6.3897e-01,  5.2324e-01,  2.2240e-01,\n          1.2210e+00, -2.7988e-01, -2.5197e-01, -1.6860e-01,  7.5415e-01,\n          1.1760e-01, -8.9583e-01,  5.1356e-01, -6.5673e-02, -5.6149e-01,\n          2.6028e-01,  3.9886e-02,  6.1276e-01, -2.1618e+00,  2.9267e-01,\n          5.5530e-01, -8.0433e-02,  1.4066e-01, -4.2263e-01, -8.8811e-01,\n         -2.3401e-01, -5.7075e-02,  1.2590e+00, -2.1011e-01,  7.0889e-01,\n          1.0105e+00, -3.2346e-01,  1.4775e+00,  6.4278e-01,  8.4505e-01,\n          6.4056e-01,  6.5547e-01,  1.2321e+00,  2.3757e-01,  1.2148e-01,\n          1.0450e+00,  8.7714e-01,  7.7429e-01, -1.5602e+00,  1.1585e+00,\n         -4.0160e-01, -6.3678e-01,  3.4250e-01,  9.9707e-02,  7.9552e-01,\n          1.3374e+00,  1.5786e+00,  1.0616e+00,  3.1501e-01, -5.9668e-01,\n          2.4873e+00,  6.2965e-01, -1.7519e-01, -5.9717e-01,  1.1048e+00,\n         -7.3185e-01, -4.7124e-01, -2.7968e-01, -5.5088e-01, -3.7158e-01,\n          2.9795e-02, -4.8636e-01,  7.1649e-01,  1.5577e+00, -7.9114e-02,\n          8.6584e-01, -1.0637e+00,  6.5250e-01,  1.3775e+00, -1.1238e-01,\n         -6.7228e-01,  2.2001e-01,  4.3770e-01,  3.3015e-01,  9.9955e-01,\n          3.1102e-01,  2.4942e-01,  9.9706e-01,  7.6046e-02,  1.3578e+00,\n          3.3440e-01, -1.4664e-01, -8.9217e-01,  4.9934e-01, -5.6033e-01,\n          8.3955e-01, -5.8100e-01,  1.0525e+00, -6.0958e-01, -7.9845e-02,\n          1.0399e+00, -2.0717e-01, -1.6016e-01,  9.4199e-01, -3.7375e-01,\n          8.5439e-01, -1.5215e+00, -2.2879e-01,  5.2327e-01,  6.5902e-01,\n          1.2471e+00,  6.2523e-01,  1.7581e+00, -2.2470e+00,  8.3274e-01,\n         -8.0447e-01,  1.2071e+00,  1.4455e-01,  2.0504e+00,  7.2916e-01,\n         -8.1949e-01, -5.2074e-01, -1.0583e+00,  2.1395e+00,  7.1008e-01,\n         -6.9936e-02,  3.3896e-01, -2.0614e-01,  5.1723e-01,  6.6576e-02,\n         -3.4120e+00,  2.4421e-01,  3.4753e-01, -4.0984e-01, -7.9627e-01,\n          3.3337e-01,  1.0844e+00,  1.4597e+00, -4.2433e-01, -4.1942e-01,\n         -6.2183e-01,  5.2887e-01,  1.4210e+00,  1.2998e+00, -1.1050e+00,\n         -3.3289e-01,  1.2766e+00, -7.1107e-01,  1.0239e+00,  5.5714e-01,\n          7.5531e-01,  8.1535e-01,  3.0360e-01,  1.6136e-01,  8.1169e-01,\n         -6.4749e-01, -1.2065e+00,  7.2022e-01, -1.0532e+00,  1.1766e+00,\n          1.9987e-01,  3.1706e-01, -1.0019e+00,  6.7641e-01, -1.4268e+00,\n          1.1168e+00,  1.1475e-01, -1.4217e+00, -6.1950e-01, -1.0298e+00,\n         -1.2098e+00,  1.1347e-01,  3.8764e-01,  3.7638e-01,  1.1620e+00,\n         -9.7189e-01, -7.0614e-01, -1.7737e-01, -5.0419e-01,  2.3377e-01,\n         -1.7344e-01, -8.3590e-01,  1.5964e+00, -3.9453e+00, -1.0746e+00,\n         -4.1148e-01,  8.5970e-01,  1.9781e-01, -5.6949e-01,  1.0020e+00,\n          3.2932e-01,  5.6258e-01,  3.7582e-01,  2.4599e-02,  1.0100e+00,\n          8.4279e-01, -3.2242e-01,  1.7630e-01, -5.3833e-01, -4.5650e-01,\n          1.2065e+00,  1.8948e-01,  1.8550e+00,  1.8852e+00, -3.4186e-01,\n          7.1368e-01,  2.0941e-01,  9.5506e-01,  1.3890e+00,  6.3844e-01,\n          6.3754e-01,  5.8900e-01,  1.3570e+00, -1.0732e+00,  2.4887e-01,\n          8.4452e-01,  1.3756e-01,  2.3544e+00, -9.8379e-01, -3.9537e-01,\n         -1.1212e-01, -1.8079e-01,  1.2443e-01, -4.5744e-01,  3.7930e-01,\n          4.5043e-01,  8.1595e-01,  9.3743e-01, -8.9952e-02,  5.7844e-01,\n         -8.5192e-01, -1.0751e-01, -9.8148e-01,  9.1740e-01,  3.5464e-01,\n         -3.6179e-02, -1.0132e+00, -1.1715e+00, -3.1527e-01, -4.1342e-01,\n          1.3690e-01, -9.4251e-01,  1.0666e+00,  1.1052e+00,  6.0932e-01,\n          3.8408e-02,  1.4812e+00,  6.4474e-02]],\n       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T20:37:01.721905Z",
     "start_time": "2024-05-08T20:37:01.694585Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "imagenet_a_directory = 'datasets/imagenet-a'\n",
    "# use target transformation for label changin if needed\n",
    "\n",
    "# Dataset: ImageNet-A\n",
    "# todo do we need to pass processor as transform?\n",
    "imagenet_a = torchvision.datasets.ImageFolder(root=imagenet_a_directory,transform=torchvision.transforms.ToTensor())"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T17:06:31.079723Z",
     "start_time": "2024-05-08T17:06:31.043535Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "imagenet_a_dataloader = torch.utils.data.DataLoader(imagenet_a, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# is it needed?"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T17:06:32.775029Z",
     "start_time": "2024-05-08T17:06:32.767637Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MeanEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        # compute mean entropy\n",
    "        # todo needs to be tested\n",
    "        logits = outputs - outputs.logsumexp(dim=-1, keepdim=True)\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0])\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "        mean_entropy= -(avg_logits * torch.exp(avg_logits)).sum(dim=-1), avg_logits\n",
    "        return mean_entropy\n",
    "\n",
    "\n",
    "loss_fn = MeanEntropy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "augmentations = []\n",
    "    # # todo what are the augementation: AugMix\n",
    "    # probably in utils.train_helpers.prepare_test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def memo_tta(pretrained_model, sample, optimizer):\n",
    "    augmented_samples=[]\n",
    "    # todo how to parallelize this\n",
    "    for augmentation in augmentations:\n",
    "        augmented_sample = augmentation(sample)\n",
    "        augmented_samples.append(augmented_sample)\n",
    "   \n",
    "    output_distributions=[]\n",
    "    \n",
    "    pretrained_model.eval()\n",
    "    for augmented_sample in augmented_samples:\n",
    "        output_distribution = pretrained_model(augmented_sample)\n",
    "        output_distributions.append(output_distribution)\n",
    "    \n",
    "    loss = loss_fn(output_distributions)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # todo Adapting BN statistics\n",
    "    return pretrained_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# todo what is the lr in paper\n",
    "learning_rate = .1\n",
    "\n",
    "def reset_model(pretrained_model): \n",
    "    pretrained_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    optimizer = torch.optim.SGD(pretrained_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return pretrained_model, optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of: test time adaptation via MEMO\n",
    "\n",
    "# \"only one gradient step per test point\" as paper\n",
    "epoch_for_adaptation = 1\n",
    "\n",
    "\n",
    "for sample in imagenet_a:\n",
    "    pretrained_model, optimizer = reset_model()\n",
    "    adapted_model = None\n",
    "    for epoch  in range(epoch_for_adaptation):\n",
    "        adapted_model = memo_tta(pretrained_model, sample, optimizer)\n",
    "    \n",
    "    # inference phase\n",
    "    with torch.no_grad():\n",
    "        prediction = adapted_model(sample['source'])\n",
    "        loss = loss_for_top_1_acc(prediction, sample['target'])\n",
    "    # log inference accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
