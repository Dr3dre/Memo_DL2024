{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEMO Assignment\n",
    "Andrea De Carlo and Pooya Torabi"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T20:47:04.364826Z",
     "start_time": "2024-05-09T20:47:04.354786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T20:47:04.737844Z",
     "start_time": "2024-05-09T20:47:04.706552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "def plot(imgs, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            boxes = None\n",
    "            masks = None\n",
    "            if isinstance(img, tuple):\n",
    "                img, target = img\n",
    "                if isinstance(target, dict):\n",
    "                    boxes = target.get(\"boxes\")\n",
    "                    masks = target.get(\"masks\")\n",
    "                elif isinstance(target, torchvision.tv_tensors.BoundingBoxes):\n",
    "                    boxes = target\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected target type: {type(target)}\")\n",
    "            img = F.to_image(img)\n",
    "            if img.dtype.is_floating_point and img.min() < 0:\n",
    "                # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "                # is useful for images coming out of Normalize()\n",
    "                img -= img.min()\n",
    "                img /= img.max()\n",
    "\n",
    "            img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "            if boxes is not None:\n",
    "                img = draw_bounding_boxes(img, boxes, colors=\"yellow\", width=3)\n",
    "            if masks is not None:\n",
    "                img = draw_segmentation_masks(img, masks.to(torch.bool), colors=[\"green\"] * masks.shape[0], alpha=.65)\n",
    "\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy(), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T21:27:07.295573Z",
     "start_time": "2024-05-09T21:27:07.203683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "            \n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, key = self.instances[idx]\n",
    "            \n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "            \n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return img, self.class_to_idx[label]"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T20:47:05.134610Z",
     "start_time": "2024-05-09T20:47:05.118971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resnet_preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset: ImageNet-A\n",
    "# https://github.com/hendrycks/natural-adv-examples\n",
    "\n",
    "# TODO read from aws s3 when on aws\n",
    "imagenet_a_directory = 'datasets/imagenet-a'\n",
    "imagenet_a = torchvision.datasets.ImageFolder(root=imagenet_a_directory,transform=resnet_preprocess)\n",
    "# imagenet_a = S3ImageFolder(root=\"OfficeHomeDataset_10072016/Real World\", transform=resnet_preprocess)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T20:47:05.452706Z",
     "start_time": "2024-05-09T20:47:05.403540Z"
    }
   },
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MeanEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        # compute mean entropy\n",
    "        # todo needs to be tested\n",
    "        logits = outputs - outputs.logsumexp(dim=-1, keepdim=True)\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0])\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "        mean_entropy= -(avg_logits * torch.exp(avg_logits)).sum(dim=-1), avg_logits\n",
    "        return mean_entropy\n",
    "\n",
    "\n",
    "loss_fn = MeanEntropy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T20:47:06.275880Z",
     "start_time": "2024-05-09T20:47:06.260216Z"
    }
   },
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "source": [
    "preaugment = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "\n",
    "augmentations = [preaugment]\n",
    "    # # todo what are the augementation: AugMix\n",
    "    # probably in utils.train_helpers.prepare_test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T20:47:06.772485Z",
     "start_time": "2024-05-09T20:47:06.740780Z"
    }
   },
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "source": [
    "def memo_tta(pretrained_model, sample, optimizer):\n",
    "    augmented_samples=[]\n",
    "    # todo how to parallelize this\n",
    "    for augmentation in augmentations:\n",
    "        augmented_sample = augmentation(sample)\n",
    "        augmented_samples.append(augmented_sample)\n",
    "\n",
    "    output_distributions=[]\n",
    "    \n",
    "    pretrained_model.eval()\n",
    "    for augmented_sample in augmented_samples:\n",
    "        output_distribution = pretrained_model(augmented_sample)\n",
    "        output_distributions.append(output_distribution[0])\n",
    "    output_distributions = torch.stack(output_distributions)\n",
    "        \n",
    "    loss, _ = loss_fn(output_distributions)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # todo Adapting BN statistics\n",
    "    return pretrained_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T21:12:29.636327Z",
     "start_time": "2024-05-09T21:12:29.620565Z"
    }
   },
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "source": [
    "# todo what is the lr in paper\n",
    "learning_rate = .1\n",
    "\n",
    "fresh_pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "def get_fresh_pretrained_model(): \n",
    "    pretrained_model = fresh_pretrained_model\n",
    "    pretrained_model = pretrained_model.cuda()\n",
    "    # in case multiple GPUs are available, this will try to utilize by Data Parallelism\n",
    "    pretrained_model = nn.DataParallel(pretrained_model)\n",
    "    optimizer = torch.optim.SGD(pretrained_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return pretrained_model, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T21:11:41.234526Z",
     "start_time": "2024-05-09T21:11:41.121269Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\pooya/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T21:14:17.373989Z",
     "start_time": "2024-05-09T21:14:12.899127Z"
    }
   },
   "source": [
    "# implementation of: test time adaptation via MEMO\n",
    "\n",
    "# \"only one gradient step per test point\" as paper\n",
    "epoch_for_adaptation = 1\n",
    "\n",
    "for sample in imagenet_a:\n",
    "    image_input = sample[0].unsqueeze(0) # create a mini-batch as expected by the resnet model\n",
    "    \n",
    "    pretrained_model, optimizer = get_fresh_pretrained_model()\n",
    "    adapted_model = None\n",
    "    for epoch  in range(epoch_for_adaptation):\n",
    "        adapted_model = memo_tta(pretrained_model, image_input, optimizer)\n",
    "    \n",
    "    # inference phase\n",
    "    with torch.no_grad():\n",
    "        prediction = adapted_model(image_input)\n",
    "        # loss = loss_for_top_1_acc(prediction, sample['target'])\n",
    "    # log inference accuracy"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m adapted_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch  \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch_for_adaptation):\n\u001B[1;32m---> 12\u001B[0m     adapted_model \u001B[38;5;241m=\u001B[39m \u001B[43mmemo_tta\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# inference phase\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "Cell \u001B[1;32mIn[79], line 19\u001B[0m, in \u001B[0;36mmemo_tta\u001B[1;34m(pretrained_model, sample, optimizer)\u001B[0m\n\u001B[0;32m     17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 19\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# todo Adapting BN statistics\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pretrained_model\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Memo_DL2024\\lib\\site-packages\\torch\\_compile.py:24\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mdisable(fn, recursive)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Memo_DL2024\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001B[0m, in \u001B[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    449\u001B[0m prior \u001B[38;5;241m=\u001B[39m set_eval_frame(callback)\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    453\u001B[0m     set_eval_frame(prior)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Memo_DL2024\\lib\\site-packages\\torch\\optim\\optimizer.py:825\u001B[0m, in \u001B[0;36mOptimizer.zero_grad\u001B[1;34m(self, set_to_none)\u001B[0m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    824\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m set_to_none:\n\u001B[1;32m--> 825\u001B[0m         p\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    826\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mgrad_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
