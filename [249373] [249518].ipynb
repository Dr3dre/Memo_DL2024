{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Adaptation via MEMO\n",
    " Pooya Torabi, Andrea De Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment with different techniques for Test Time Adaptation (TTA) on the ImageNet-A dataset. We will experiment with the following techniques:\n",
    "- [Marginal Entropy Minimization with One test point (MEMO)](https://arxiv.org/pdf/2110.09506)\n",
    "- MEMO + Batch Normalization on single test point\n",
    "- Using the augmentation \"Random Patch Hiding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time Adaptation on single test point\n",
    "Test Time Adaptation (TTA) is a technique to improve the generalization of a model by adapting the model at test time. Among different techniques proposed, we are interested in Entropy Minimization and using only a single test point for adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installing dependencies and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.259863Z",
     "start_time": "2024-07-18T09:36:07.200393Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.275934Z",
     "start_time": "2024-07-18T09:36:12.260892Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from io import BytesIO\n",
    "from os import getenv\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import ImageOps, Image\n",
    "from dotenv import load_dotenv\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "USE_WANDB = False\n",
    "\n",
    "if USE_WANDB == True:\n",
    "    %pip install wandb\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.291581Z",
     "start_time": "2024-07-18T09:36:12.275934Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to plot images\n",
    "\n",
    "def plot(images):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(images), figsize=(20, 10))\n",
    "    for i, image in enumerate(images):\n",
    "        # Poor man's re-normalization for the colors to be OK-ish. This\n",
    "        # is useful for images coming out of Normalize()\n",
    "        image -= image.min()\n",
    "        image /= image.max()\n",
    "\n",
    "        ax[i].imshow(image.permute(1, 2, 0))\n",
    "        ax[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilities for loading and preprocessing ImageNet-A dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.323350Z",
     "start_time": "2024-07-18T09:36:12.293315Z"
    }
   },
   "outputs": [],
   "source": [
    "## from Lab 2 - Finetune AlexNet (S3 bucket) v2.ipynb\n",
    "\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, limit=None, transform=None):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "\n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        if limit:\n",
    "            self.instances = self.instances[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label, key = self.instances[idx]\n",
    "\n",
    "            # Download image from S3\n",
    "            # response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            img_bytes = BytesIO()\n",
    "            response = self.s3_client.download_fileobj(Bucket=self.s3_bucket, Key=key, Fileobj=img_bytes)\n",
    "            # img_bytes = response[\"Body\"]._raw_stream.data\n",
    "\n",
    "            # Open image with PIL\n",
    "            img = Image.open(img_bytes).convert(\"RGB\")\n",
    "\n",
    "            # Apply transformations if any\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading image at index {idx}: {str(e)}\")\n",
    "\n",
    "        return img, self.class_to_idx[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.335509Z",
     "start_time": "2024-07-18T09:36:12.324650Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "\n",
    "# Using the mean and std of Imagenet dataset\n",
    "pre_process_mean = [0.485, 0.456, 0.406]        \n",
    "pre_process_std = [0.229, 0.224, 0.225]\n",
    "normalize = torchvision.transforms.Normalize(mean=pre_process_mean, std=pre_process_std)\n",
    "resnet_preprocess = torchvision.transforms.Compose([  # ResNet-50 expects 224x224 images\n",
    "    torchvision.transforms.Resize(256),      # Resize the image to 256x256\n",
    "    torchvision.transforms.CenterCrop(224),  # Crop the center 224x224 square\n",
    "    torchvision.transforms.ToTensor(),       # Convert the image to a tensor with pixels in the range [0, 1]\n",
    "    normalize\n",
    "])\n",
    "\n",
    "LIMIT = 3000     # for debugging, default = None\n",
    "IN_version = \"a\"    # or v2\n",
    "\n",
    "if IN_version == \"a\":\n",
    "    name_data = \"imagenet-a\"\n",
    "    print(\"Testing on ImageNet-A\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Prerequisite: Import imagenet-a folder on colab\n",
    "    datapath = f'/content/{name_data}'            # CHANGE with imagenet-a folder position on Google Colab\n",
    "    try:\n",
    "        dataset = torchvision.datasets.ImageFolder(root=datapath, limit=LIMIT, transform=resnet_preprocess)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Please import the imagenet-a folder on Google Colab\")\n",
    "\n",
    "else:\n",
    "    datapath = f\"{name_data}\"\n",
    "    dataset = S3ImageFolder(root=datapath, limit=LIMIT, transform=resnet_preprocess)\n",
    "\n",
    "dataset_len = len(dataset)\n",
    "print(dataset_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagenet-a contains a set of images that are similar to the original ImageNet dataset but are more challenging.\n",
    "We need a mapping 1000 ImageNet classes to the 200 ImageNet-a classes. \n",
    "We will use the mapping provided by the authors of the ImageNet-a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:25:51.074966Z",
     "start_time": "2024-07-17T12:25:50.789242Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/hendrycks/natural-adv-example\n",
    "\n",
    "# https://github.com/hendrycks/natural-adv-examples/blob/master/eval.py\n",
    "thousand_k_to_200 = {0: -1, 1: -1, 2: -1, 3: -1, 4: -1, 5: -1, 6: 0, 7: -1, 8: -1, 9: -1, 10: -1, 11: 1, 12: -1, 13: 2,\n",
    "                     14: -1, 15: 3, 16: -1, 17: 4, 18: -1, 19: -1, 20: -1, 21: -1, 22: 5, 23: 6, 24: -1, 25: -1, 26: -1,\n",
    "                     27: 7, 28: -1, 29: -1, 30: 8, 31: -1, 32: -1, 33: -1, 34: -1, 35: -1, 36: -1, 37: 9, 38: -1,\n",
    "                     39: 10, 40: -1, 41: -1, 42: 11, 43: -1, 44: -1, 45: -1, 46: -1, 47: 12, 48: -1, 49: -1, 50: 13,\n",
    "                     51: -1, 52: -1, 53: -1, 54: -1, 55: -1, 56: -1, 57: 14, 58: -1, 59: -1, 60: -1, 61: -1, 62: -1,\n",
    "                     63: -1, 64: -1, 65: -1, 66: -1, 67: -1, 68: -1, 69: -1, 70: 15, 71: 16, 72: -1, 73: -1, 74: -1,\n",
    "                     75: -1, 76: 17, 77: -1, 78: -1, 79: 18, 80: -1, 81: -1, 82: -1, 83: -1, 84: -1, 85: -1, 86: -1,\n",
    "                     87: -1, 88: -1, 89: 19, 90: 20, 91: -1, 92: -1, 93: -1, 94: 21, 95: -1, 96: 22, 97: 23, 98: -1,\n",
    "                     99: 24, 100: -1, 101: -1, 102: -1, 103: -1, 104: -1, 105: 25, 106: -1, 107: 26, 108: 27, 109: -1,\n",
    "                     110: 28, 111: -1, 112: -1, 113: 29, 114: -1, 115: -1, 116: -1, 117: -1, 118: -1, 119: -1, 120: -1,\n",
    "                     121: -1, 122: -1, 123: -1, 124: 30, 125: 31, 126: -1, 127: -1, 128: -1, 129: -1, 130: 32, 131: -1,\n",
    "                     132: 33, 133: -1, 134: -1, 135: -1, 136: -1, 137: -1, 138: -1, 139: -1, 140: -1, 141: -1, 142: -1,\n",
    "                     143: 34, 144: 35, 145: -1, 146: -1, 147: -1, 148: -1, 149: -1, 150: 36, 151: 37, 152: -1, 153: -1,\n",
    "                     154: -1, 155: -1, 156: -1, 157: -1, 158: -1, 159: -1, 160: -1, 161: -1, 162: -1, 163: -1, 164: -1,\n",
    "                     165: -1, 166: -1, 167: -1, 168: -1, 169: -1, 170: -1, 171: -1, 172: -1, 173: -1, 174: -1, 175: -1,\n",
    "                     176: -1, 177: -1, 178: -1, 179: -1, 180: -1, 181: -1, 182: -1, 183: -1, 184: -1, 185: -1, 186: -1,\n",
    "                     187: -1, 188: -1, 189: -1, 190: -1, 191: -1, 192: -1, 193: -1, 194: -1, 195: -1, 196: -1, 197: -1,\n",
    "                     198: -1, 199: -1, 200: -1, 201: -1, 202: -1, 203: -1, 204: -1, 205: -1, 206: -1, 207: 38, 208: -1,\n",
    "                     209: -1, 210: -1, 211: -1, 212: -1, 213: -1, 214: -1, 215: -1, 216: -1, 217: -1, 218: -1, 219: -1,\n",
    "                     220: -1, 221: -1, 222: -1, 223: -1, 224: -1, 225: -1, 226: -1, 227: -1, 228: -1, 229: -1, 230: -1,\n",
    "                     231: -1, 232: -1, 233: -1, 234: 39, 235: 40, 236: -1, 237: -1, 238: -1, 239: -1, 240: -1, 241: -1,\n",
    "                     242: -1, 243: -1, 244: -1, 245: -1, 246: -1, 247: -1, 248: -1, 249: -1, 250: -1, 251: -1, 252: -1,\n",
    "                     253: -1, 254: 41, 255: -1, 256: -1, 257: -1, 258: -1, 259: -1, 260: -1, 261: -1, 262: -1, 263: -1,\n",
    "                     264: -1, 265: -1, 266: -1, 267: -1, 268: -1, 269: -1, 270: -1, 271: -1, 272: -1, 273: -1, 274: -1,\n",
    "                     275: -1, 276: -1, 277: 42, 278: -1, 279: -1, 280: -1, 281: -1, 282: -1, 283: 43, 284: -1, 285: -1,\n",
    "                     286: -1, 287: 44, 288: -1, 289: -1, 290: -1, 291: 45, 292: -1, 293: -1, 294: -1, 295: 46, 296: -1,\n",
    "                     297: -1, 298: 47, 299: -1, 300: -1, 301: 48, 302: -1, 303: -1, 304: -1, 305: -1, 306: 49, 307: 50,\n",
    "                     308: 51, 309: 52, 310: 53, 311: 54, 312: -1, 313: 55, 314: 56, 315: 57, 316: -1, 317: 58, 318: -1,\n",
    "                     319: 59, 320: -1, 321: -1, 322: -1, 323: 60, 324: 61, 325: -1, 326: 62, 327: 63, 328: -1, 329: -1,\n",
    "                     330: 64, 331: -1, 332: -1, 333: -1, 334: 65, 335: 66, 336: 67, 337: -1, 338: -1, 339: -1, 340: -1,\n",
    "                     341: -1, 342: -1, 343: -1, 344: -1, 345: -1, 346: -1, 347: 68, 348: -1, 349: -1, 350: -1, 351: -1,\n",
    "                     352: -1, 353: -1, 354: -1, 355: -1, 356: -1, 357: -1, 358: -1, 359: -1, 360: -1, 361: 69, 362: -1,\n",
    "                     363: 70, 364: -1, 365: -1, 366: -1, 367: -1, 368: -1, 369: -1, 370: -1, 371: -1, 372: 71, 373: -1,\n",
    "                     374: -1, 375: -1, 376: -1, 377: -1, 378: 72, 379: -1, 380: -1, 381: -1, 382: -1, 383: -1, 384: -1,\n",
    "                     385: -1, 386: 73, 387: -1, 388: -1, 389: -1, 390: -1, 391: -1, 392: -1, 393: -1, 394: -1, 395: -1,\n",
    "                     396: -1, 397: 74, 398: -1, 399: -1, 400: 75, 401: 76, 402: 77, 403: -1, 404: 78, 405: -1, 406: -1,\n",
    "                     407: 79, 408: -1, 409: -1, 410: -1, 411: 80, 412: -1, 413: -1, 414: -1, 415: -1, 416: 81, 417: 82,\n",
    "                     418: -1, 419: -1, 420: 83, 421: -1, 422: -1, 423: -1, 424: -1, 425: 84, 426: -1, 427: -1, 428: 85,\n",
    "                     429: -1, 430: 86, 431: -1, 432: -1, 433: -1, 434: -1, 435: -1, 436: -1, 437: 87, 438: 88, 439: -1,\n",
    "                     440: -1, 441: -1, 442: -1, 443: -1, 444: -1, 445: 89, 446: -1, 447: -1, 448: -1, 449: -1, 450: -1,\n",
    "                     451: -1, 452: -1, 453: -1, 454: -1, 455: -1, 456: 90, 457: 91, 458: -1, 459: -1, 460: -1, 461: 92,\n",
    "                     462: 93, 463: -1, 464: -1, 465: -1, 466: -1, 467: -1, 468: -1, 469: -1, 470: 94, 471: -1, 472: 95,\n",
    "                     473: -1, 474: -1, 475: -1, 476: -1, 477: -1, 478: -1, 479: -1, 480: -1, 481: -1, 482: -1, 483: 96,\n",
    "                     484: -1, 485: -1, 486: 97, 487: -1, 488: 98, 489: -1, 490: -1, 491: -1, 492: 99, 493: -1, 494: -1,\n",
    "                     495: -1, 496: 100, 497: -1, 498: -1, 499: -1, 500: -1, 501: -1, 502: -1, 503: -1, 504: -1, 505: -1,\n",
    "                     506: -1, 507: -1, 508: -1, 509: -1, 510: -1, 511: -1, 512: -1, 513: -1, 514: 101, 515: -1,\n",
    "                     516: 102, 517: -1, 518: -1, 519: -1, 520: -1, 521: -1, 522: -1, 523: -1, 524: -1, 525: -1, 526: -1,\n",
    "                     527: -1, 528: 103, 529: -1, 530: 104, 531: -1, 532: -1, 533: -1, 534: -1, 535: -1, 536: -1,\n",
    "                     537: -1, 538: -1, 539: 105, 540: -1, 541: -1, 542: 106, 543: 107, 544: -1, 545: -1, 546: -1,\n",
    "                     547: -1, 548: -1, 549: 108, 550: -1, 551: -1, 552: 109, 553: -1, 554: -1, 555: -1, 556: -1,\n",
    "                     557: 110, 558: -1, 559: -1, 560: -1, 561: 111, 562: 112, 563: -1, 564: -1, 565: -1, 566: -1,\n",
    "                     567: -1, 568: -1, 569: 113, 570: -1, 571: -1, 572: 114, 573: 115, 574: -1, 575: 116, 576: -1,\n",
    "                     577: -1, 578: -1, 579: 117, 580: -1, 581: -1, 582: -1, 583: -1, 584: -1, 585: -1, 586: -1, 587: -1,\n",
    "                     588: -1, 589: 118, 590: -1, 591: -1, 592: -1, 593: -1, 594: -1, 595: -1, 596: -1, 597: -1, 598: -1,\n",
    "                     599: -1, 600: -1, 601: -1, 602: -1, 603: -1, 604: -1, 605: -1, 606: 119, 607: 120, 608: -1,\n",
    "                     609: 121, 610: -1, 611: -1, 612: -1, 613: -1, 614: 122, 615: -1, 616: -1, 617: -1, 618: -1,\n",
    "                     619: -1, 620: -1, 621: -1, 622: -1, 623: -1, 624: -1, 625: -1, 626: 123, 627: 124, 628: -1,\n",
    "                     629: -1, 630: -1, 631: -1, 632: -1, 633: -1, 634: -1, 635: -1, 636: -1, 637: -1, 638: -1, 639: -1,\n",
    "                     640: 125, 641: 126, 642: 127, 643: 128, 644: -1, 645: -1, 646: -1, 647: -1, 648: -1, 649: -1,\n",
    "                     650: -1, 651: -1, 652: -1, 653: -1, 654: -1, 655: -1, 656: -1, 657: -1, 658: 129, 659: -1, 660: -1,\n",
    "                     661: -1, 662: -1, 663: -1, 664: -1, 665: -1, 666: -1, 667: -1, 668: 130, 669: -1, 670: -1, 671: -1,\n",
    "                     672: -1, 673: -1, 674: -1, 675: -1, 676: -1, 677: 131, 678: -1, 679: -1, 680: -1, 681: -1,\n",
    "                     682: 132, 683: -1, 684: 133, 685: -1, 686: -1, 687: 134, 688: -1, 689: -1, 690: -1, 691: -1,\n",
    "                     692: -1, 693: -1, 694: -1, 695: -1, 696: -1, 697: -1, 698: -1, 699: -1, 700: -1, 701: 135, 702: -1,\n",
    "                     703: -1, 704: 136, 705: -1, 706: -1, 707: -1, 708: -1, 709: -1, 710: -1, 711: -1, 712: -1, 713: -1,\n",
    "                     714: -1, 715: -1, 716: -1, 717: -1, 718: -1, 719: 137, 720: -1, 721: -1, 722: -1, 723: -1, 724: -1,\n",
    "                     725: -1, 726: -1, 727: -1, 728: -1, 729: -1, 730: -1, 731: -1, 732: -1, 733: -1, 734: -1, 735: -1,\n",
    "                     736: 138, 737: -1, 738: -1, 739: -1, 740: -1, 741: -1, 742: -1, 743: -1, 744: -1, 745: -1,\n",
    "                     746: 139, 747: -1, 748: -1, 749: 140, 750: -1, 751: -1, 752: 141, 753: -1, 754: -1, 755: -1,\n",
    "                     756: -1, 757: -1, 758: 142, 759: -1, 760: -1, 761: -1, 762: -1, 763: 143, 764: -1, 765: 144,\n",
    "                     766: -1, 767: -1, 768: 145, 769: -1, 770: -1, 771: -1, 772: -1, 773: 146, 774: 147, 775: -1,\n",
    "                     776: 148, 777: -1, 778: -1, 779: 149, 780: 150, 781: -1, 782: -1, 783: -1, 784: -1, 785: -1,\n",
    "                     786: 151, 787: -1, 788: -1, 789: -1, 790: -1, 791: -1, 792: 152, 793: -1, 794: -1, 795: -1,\n",
    "                     796: -1, 797: 153, 798: -1, 799: -1, 800: -1, 801: -1, 802: 154, 803: 155, 804: 156, 805: -1,\n",
    "                     806: -1, 807: -1, 808: -1, 809: -1, 810: -1, 811: -1, 812: -1, 813: 157, 814: -1, 815: 158,\n",
    "                     816: -1, 817: -1, 818: -1, 819: -1, 820: 159, 821: -1, 822: -1, 823: 160, 824: -1, 825: -1,\n",
    "                     826: -1, 827: -1, 828: -1, 829: -1, 830: -1, 831: 161, 832: -1, 833: 162, 834: -1, 835: 163,\n",
    "                     836: -1, 837: -1, 838: -1, 839: 164, 840: -1, 841: -1, 842: -1, 843: -1, 844: -1, 845: 165,\n",
    "                     846: -1, 847: 166, 848: -1, 849: -1, 850: 167, 851: -1, 852: -1, 853: -1, 854: -1, 855: -1,\n",
    "                     856: -1, 857: -1, 858: -1, 859: 168, 860: -1, 861: -1, 862: 169, 863: -1, 864: -1, 865: -1,\n",
    "                     866: -1, 867: -1, 868: -1, 869: -1, 870: 170, 871: -1, 872: -1, 873: -1, 874: -1, 875: -1, 876: -1,\n",
    "                     877: -1, 878: -1, 879: 171, 880: 172, 881: -1, 882: -1, 883: -1, 884: -1, 885: -1, 886: -1,\n",
    "                     887: -1, 888: 173, 889: -1, 890: 174, 891: -1, 892: -1, 893: -1, 894: -1, 895: -1, 896: -1,\n",
    "                     897: 175, 898: -1, 899: -1, 900: 176, 901: -1, 902: -1, 903: -1, 904: -1, 905: -1, 906: -1,\n",
    "                     907: 177, 908: -1, 909: -1, 910: -1, 911: -1, 912: -1, 913: 178, 914: -1, 915: -1, 916: -1,\n",
    "                     917: -1, 918: -1, 919: -1, 920: -1, 921: -1, 922: -1, 923: -1, 924: 179, 925: -1, 926: -1, 927: -1,\n",
    "                     928: -1, 929: -1, 930: -1, 931: -1, 932: 180, 933: 181, 934: 182, 935: -1, 936: -1, 937: 183,\n",
    "                     938: -1, 939: -1, 940: -1, 941: -1, 942: -1, 943: 184, 944: -1, 945: 185, 946: -1, 947: 186,\n",
    "                     948: -1, 949: -1, 950: -1, 951: 187, 952: -1, 953: -1, 954: 188, 955: -1, 956: 189, 957: 190,\n",
    "                     958: -1, 959: 191, 960: -1, 961: -1, 962: -1, 963: -1, 964: -1, 965: -1, 966: -1, 967: -1, 968: -1,\n",
    "                     969: -1, 970: -1, 971: 192, 972: 193, 973: -1, 974: -1, 975: -1, 976: -1, 977: -1, 978: -1,\n",
    "                     979: -1, 980: 194, 981: 195, 982: -1, 983: -1, 984: 196, 985: -1, 986: 197, 987: 198, 988: 199,\n",
    "                     989: -1, 990: -1, 991: -1, 992: -1, 993: -1, 994: -1, 995: -1, 996: -1, 997: -1, 998: -1, 999: -1}\n",
    "indices_in_1k = [k for k in thousand_k_to_200 if thousand_k_to_200[k] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model\n",
    "We use ResNet models. We also need to modify BN layers, discussed in a later section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.478840Z",
     "start_time": "2024-07-18T09:36:12.463241Z"
    }
   },
   "outputs": [],
   "source": [
    "## from https://github.com/bethgelab/robustness/blob/main/robusta/batchnorm/bn.py#L175\n",
    "\n",
    "# modified batch normalization forward function\n",
    "## from # https://github.com/hendrycks/natural-adv-example\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "def modified_bn_forward(self, input):\n",
    "    est_mean = torch.zeros(self.running_mean.shape, device=self.running_mean.device)\n",
    "    est_var = torch.ones(self.running_var.shape, device=self.running_var.device)\n",
    "    nn.functional.batch_norm(input, est_mean, est_var, None, None, True, 1.0, self.eps)\n",
    "    running_mean = self.prior * self.running_mean + (1 - self.prior) * est_mean\n",
    "    running_var = self.prior * self.running_var + (1 - self.prior) * est_var\n",
    "    return nn.functional.batch_norm(input, running_mean, running_var, self.weight, self.bias, False, 0, self.eps)\n",
    "\n",
    "\n",
    "# initialize the model with the pretrained weights\n",
    "def get_fresh_pretrained_model(model=\"resnet\", BN_prior_strength=None):\n",
    "    \n",
    "    # fresh_pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True, verbose=False)\n",
    "\n",
    "    if model == \"resnet\":\n",
    "        pretrained_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    elif model == \"vit\":\n",
    "        pretrained_model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT)\n",
    "    elif model == \"resnet18\":\n",
    "        pretrained_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    pretrained_model = pretrained_model.cuda()\n",
    "    # in case multiple GPUs are available, this will try to utilize by Data Parallelism\n",
    "    pretrained_model = nn.DataParallel(pretrained_model)\n",
    "    \n",
    "    if BN_prior_strength is not None:\n",
    "        nn.BatchNorm2d.prior = float(BN_prior_strength) / float(BN_prior_strength + 1)\n",
    "        nn.BatchNorm2d.forward = modified_bn_forward\n",
    "\n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for testing an adaptation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.494863Z",
     "start_time": "2024-07-18T09:36:12.478840Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiment_adaptation_on_dataset(model, adaptation_method, **kwargs):      \n",
    "    BN_prior_strength = kwargs.get('BN_prior_strength', None)\n",
    "    use_resnet18 = kwargs.get('use_resnet18', False)\n",
    "    kwargs.pop('use_resnet18', None) # remove this key because it is not a parameter for the adaptation method\n",
    "\n",
    "    if USE_WANDB:\n",
    "        name_run = f\"{model}_adam={kwargs.get('adam', None)}_lr={kwargs.get('lr')}_aug={kwargs.get('augmentation')}_naug={kwargs.get('n_aug')}_augselec={kwargs.get('augmentation_select', False)}\"\n",
    "\n",
    "        run = wandb.init(\n",
    "            project=\"MEMO_\",\n",
    "            name = name_run,\n",
    "            config=kwargs\n",
    "        )\n",
    "\n",
    "    pbar = tqdm(range(dataset_len), desc=\"TTA on dataset\")\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for idx in pbar:\n",
    "        (input_image, input_label) = dataset[idx]\n",
    "        input_image = input_image.to(device='cuda')\n",
    "\n",
    "        pretrained_model = get_fresh_pretrained_model(model, BN_prior_strength)\n",
    "        adapted_model = adaptation_method(pretrained_model, input_image, **kwargs)\n",
    "\n",
    "        # inference phase\n",
    "        prediction = test_single(adapted_model, input_image, BN_prior_strength)\n",
    "        correctness = 1 if prediction == input_label else 0\n",
    "        correct_predictions += correctness\n",
    "\n",
    "        pbar.set_postfix({\"correct_predictions\": correct_predictions})\n",
    "\n",
    "    top1_accuracy = correct_predictions / dataset_len\n",
    "    \n",
    "    if USE_WANDB:\n",
    "        wandb.log({\"acc@1\": top1_accuracy})\n",
    "        wandb.finish()\n",
    "\n",
    "    return top1_accuracy\n",
    "\n",
    "\n",
    "def test_single(adapted_model, sample, BN_prior_strength=None):\n",
    "    \"\"\"\n",
    "    :param adapted_model: \n",
    "    :param sample: \n",
    "    :param BN_prior_strength: This optional argument is used for BN adaptation. By default, we do not do BN adaptation.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    adapted_model.eval()\n",
    "\n",
    "    if BN_prior_strength is not None:\n",
    "        nn.BatchNorm2d.prior = float(BN_prior_strength) / float(BN_prior_strength + 1)\n",
    "\n",
    "    sample = sample.unsqueeze(0)  # create a mini-batch as expected by the resnet model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = adapted_model(sample)\n",
    "        outputs = outputs[:, indices_in_1k]\n",
    "        _, predicted = outputs.max(1)\n",
    "    prediction = predicted.item()\n",
    "\n",
    "    if BN_prior_strength is not None:\n",
    "        nn.BatchNorm2d.prior = 1\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO\n",
    "In MEMO adaptation, we minimize the marginal entropy of a set augmentations from test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:33:01.156555Z",
     "start_time": "2024-07-17T12:33:01.140631Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MarginalEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        # compute mean entropy\n",
    "        logits = outputs - outputs.logsumexp(dim=-1, keepdim=True)\n",
    "        avg_logits = logits.logsumexp(dim=0) - np.log(logits.shape[0])\n",
    "        min_real = torch.finfo(avg_logits.dtype).min\n",
    "        avg_logits = torch.clamp(avg_logits, min=min_real)\n",
    "        mean_entropy = -(avg_logits * torch.exp(avg_logits)).sum(dim=-1), avg_logits\n",
    "        return mean_entropy\n",
    "\n",
    "memo_loss_fn = MarginalEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### augmentation function\n",
    "MEMO uses [AugMix](https://arxiv.org/pdf/1912.02781) for creating 64 augmentations from a single test image. Each time calling `augmix`, it applies some random augmentations from a set of augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:12.541746Z",
     "start_time": "2024-07-18T09:36:12.510500Z"
    }
   },
   "outputs": [],
   "source": [
    "# augmix from original paper\n",
    "\n",
    "preaugment = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "def augmix(x_orig):\n",
    "    x_orig = preaugment(x_orig)\n",
    "    x_orig = torchvision.transforms.ToPILImage()(x_orig)\n",
    "    x_processed = preprocess(x_orig)\n",
    "    w = np.float32(np.random.dirichlet([1.0, 1.0, 1.0]))\n",
    "    m = np.float32(np.random.beta(1.0, 1.0))\n",
    "\n",
    "    mix = torch.zeros_like(x_processed)\n",
    "    for i in range(3):\n",
    "        x_aug = x_orig.copy()\n",
    "        for _ in range(np.random.randint(1, 4)):\n",
    "            x_aug = np.random.choice(augmentations)(x_aug)\n",
    "        mix += w[i] * preprocess(x_aug)\n",
    "    mix = m * x_processed + (1 - m) * mix\n",
    "    return mix\n",
    "\n",
    "def autocontrast(pil_img, level=None):\n",
    "    return ImageOps.autocontrast(pil_img)\n",
    "\n",
    "\n",
    "def equalize(pil_img, level=None):\n",
    "    return ImageOps.equalize(pil_img)\n",
    "\n",
    "\n",
    "def rotate(pil_img, level):\n",
    "    degrees = int_parameter(rand_lvl(level), 30)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        degrees = -degrees\n",
    "    return pil_img.rotate(degrees, resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "\n",
    "def solarize(pil_img, level):\n",
    "    level = int_parameter(rand_lvl(level), 256)\n",
    "    return ImageOps.solarize(pil_img, 256 - level)\n",
    "\n",
    "\n",
    "def shear_x(pil_img, level):\n",
    "    level = float_parameter(rand_lvl(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, level, 0, 0, 1, 0), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "\n",
    "def shear_y(pil_img, level):\n",
    "    level = float_parameter(rand_lvl(level), 0.3)\n",
    "    if np.random.uniform() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, 0, 0, level, 1, 0), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "\n",
    "def translate_x(pil_img, level):\n",
    "    level = int_parameter(rand_lvl(level), 224 / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, 0, level, 0, 1, 0), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "\n",
    "def translate_y(pil_img, level):\n",
    "    level = int_parameter(rand_lvl(level), 224 / 3)\n",
    "    if np.random.random() > 0.5:\n",
    "        level = -level\n",
    "    return pil_img.transform((224, 224), Image.AFFINE, (1, 0, 0, 0, 1, level), resample=Image.BILINEAR, fillcolor=128)\n",
    "\n",
    "\n",
    "def posterize(pil_img, level):\n",
    "    level = int_parameter(rand_lvl(level), 4)\n",
    "    return ImageOps.posterize(pil_img, 4 - level)\n",
    "\n",
    "\n",
    "def int_parameter(level, maxval):\n",
    "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
    "    Args:\n",
    "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
    "    maxval: Maximum value that the operation can have. This will be scaled\n",
    "      to level/PARAMETER_MAX.\n",
    "    Returns:\n",
    "    An int that results from scaling `maxval` according to `level`.\n",
    "    \"\"\"\n",
    "    return int(level * maxval / 10)\n",
    "\n",
    "\n",
    "def float_parameter(level, maxval):\n",
    "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
    "    Args:\n",
    "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
    "    maxval: Maximum value that the operation can have. This will be scaled\n",
    "      to level/PARAMETER_MAX.\n",
    "    Returns:\n",
    "    A float that results from scaling `maxval` according to `level`.\n",
    "    \"\"\"\n",
    "    return float(level) * maxval / 10.\n",
    "\n",
    "\n",
    "def rand_lvl(n):\n",
    "    return np.random.uniform(low=0.1, high=n)\n",
    "\n",
    "\n",
    "augmentations = [\n",
    "    autocontrast,\n",
    "    equalize,\n",
    "    lambda x: rotate(x, 1),\n",
    "    lambda x: solarize(x, 1),\n",
    "    lambda x: shear_x(x, 1),\n",
    "    lambda x: shear_y(x, 1),\n",
    "    lambda x: translate_x(x, 1),\n",
    "    lambda x: translate_y(x, 1),\n",
    "    lambda x: posterize(x, 1),\n",
    "]\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "preaugment = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of this augmentation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:13.068470Z",
     "start_time": "2024-07-18T09:36:12.541746Z"
    }
   },
   "outputs": [],
   "source": [
    "augmix_example_img = []\n",
    "sample_image = dataset[1][0]\n",
    "augmix_example_img.append(sample_image)\n",
    "for i in range(3):\n",
    "    augmix_example_img.append(augmix(sample_image))\n",
    "\n",
    "plot(augmix_example_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO implementation\n",
    "There are a few choices that we replicate from the original paper:\n",
    "- 64 augmentations per test point\n",
    "- 1 epoch for adaptation per test point\n",
    "- learning rate of 0.00001\n",
    "- SGD optimizer for ResNet models\n",
    "- Prior strength of 16 for single sample batch normalization\n",
    "\n",
    "We create optional `BN_prior_strength` parameter to enable single sample batch normalization, optionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:33:02.326584Z",
     "start_time": "2024-07-17T12:33:02.310631Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memo_tta(pretrained_model, sample, augmentation=augmix, n_aug=8, lr=5e-5, adam=False, augmentation_selection=False, BN_prior_strength=None):\n",
    "    \"\"\"\n",
    "    :param pretrained_model: \n",
    "    :param sample: \n",
    "    :param BN_prior_strength: This optional argument is for BN adaptation. By default, we do not do BN adaptation.\n",
    "    :return: adapted model using MEMO technique\n",
    "    \"\"\"\n",
    "    epochs_for_adaptation = 1\n",
    "    \n",
    "    if adam:\n",
    "        optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(pretrained_model.parameters(), lr=lr)\n",
    "\n",
    "    augmented_samples = []\n",
    "    for _ in range(n_aug):\n",
    "        augmented_sample = augmentation(sample)\n",
    "        augmented_samples.append(augmented_sample)\n",
    "    augmented_samples = torch.stack(augmented_samples)\n",
    "\n",
    "    pretrained_model.eval()\n",
    "\n",
    "    # TODO: what is this doing?\n",
    "    if augmentation_selection:\n",
    "        with torch.no_grad():\n",
    "            output_distributions = pretrained_model(augmented_samples)\n",
    "            output_distributions = output_distributions[:, indices_in_1k]\n",
    "            output_distributions = torch.softmax(output_distributions, dim=-1)\n",
    "\n",
    "            output_entropies = []\n",
    "            for output_distribution in output_distributions:\n",
    "                output_entropy = Categorical(probs=output_distribution).entropy()\n",
    "                output_entropies.append(output_entropy)\n",
    "            output_entropies = torch.stack(output_entropies).cpu()\n",
    "\n",
    "            augmentation_selection_threshold = np.percentile(output_entropies, 50)\n",
    "\n",
    "            augmentation_idx_to_keep = []\n",
    "            for idx, output_entropy in enumerate(output_entropies):\n",
    "                if output_entropy <= augmentation_selection_threshold:\n",
    "                    augmentation_idx_to_keep.append(idx)\n",
    "\n",
    "            augmented_samples = augmented_samples[augmentation_idx_to_keep]\n",
    "\n",
    "    if BN_prior_strength is not None:\n",
    "        nn.BatchNorm2d.prior = float(BN_prior_strength) / float(BN_prior_strength + 1)\n",
    "\n",
    "    for epoch in range(epochs_for_adaptation):\n",
    "        output_distributions = pretrained_model(augmented_samples)\n",
    "        output_distributions = output_distributions[:, indices_in_1k]\n",
    "\n",
    "        loss, _ = memo_loss_fn(output_distributions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if USE_WANDB:\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    if BN_prior_strength is not None:\n",
    "        nn.BatchNorm2d.prior = 1\n",
    "    \n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single point BN adaptation\n",
    "Zhang et al. (2022) cited the work of Schneider et al. (2020) that BN adaptation can be effective even with only one test point. Hence, MEMO utilized this technique on top of entropy minimization.\n",
    "BN adaptation is also utilized in another recent work on single point TTA, [SITA](https://computer-vision-in-the-wild.github.io/cvpr-2023/static/cvpr2023/accepted_papers/5/CameraReady/cvinw_camera_ready.pdf); hence, it signifies the importance of BN for handling distribution shift in test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "For baseline, we test the pretrained model without any adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:33:04.163617Z",
     "start_time": "2024-07-17T12:33:04.132190Z"
    }
   },
   "outputs": [],
   "source": [
    "# implementation of: baseline, pretrained model, without test time adaptation\n",
    "def no_adaptation(pretrained_model, input_image):\n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Project Assignment's results, we get the baseline accuracy with Random Resized Crop transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomResizedCrop = torchvision.transforms.RandomResizedCrop(size=224)\n",
    "\n",
    "random_cropped_img = []\n",
    "for i in range(8):\n",
    "    random_cropped_img.append(RandomResizedCrop(dataset[0][0]))\n",
    "\n",
    "plot(random_cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Random Patch Hiding Augmentation function\n",
    "Inspired by a recent [technique](https://openreview.net/pdf?id=SHMi1b7sjXk#page=6.63) for single sample TTA, we also experiment with the random patch hiding augmentation. This augmentation was proposed first [here](https://arxiv.org/pdf/1704.04232), although not for TTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:33:05.854222Z",
     "start_time": "2024-07-17T12:33:05.822596Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_hide_patch(img):\n",
    "    # get width and height of the image\n",
    "    _, H, W = img.shape\n",
    "\n",
    "    patch_size = int(min(H, W) * 0.1)\n",
    "\n",
    "    # Create a copy of the image to avoid modifying the original image\n",
    "    masked_img = img.clone()\n",
    "\n",
    "    hide_prob = 0.5\n",
    "\n",
    "    for x in range(0, W, patch_size):\n",
    "        for y in range(0, H, patch_size):\n",
    "            x_end = min(W, x + patch_size)\n",
    "            y_end = min(H, y + patch_size)\n",
    "            choose_to_hide = random.uniform(0, 1)\n",
    "            if choose_to_hide <= hide_prob:\n",
    "                masked_img[:, y:y_end, x:x_end] = 0\n",
    "\n",
    "    return masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of this augmentation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:33:07.592833Z",
     "start_time": "2024-07-17T12:33:07.133137Z"
    }
   },
   "outputs": [],
   "source": [
    "random_patched_img = []\n",
    "for i in range(3):\n",
    "    random_patched_img.append(random_hide_patch(dataset[0][0]))\n",
    "\n",
    "plot(random_patched_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on ViT_16_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T09:36:15.210368Z",
     "start_time": "2024-07-18T09:36:13.588157Z"
    }
   },
   "outputs": [],
   "source": [
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=no_adaptation)\n",
    "\n",
    "print(f\"top-1 accuracy for pretrained model without TTA: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline of E. Ricci\n",
    "\n",
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_accuracy = 0\n",
    "for lr in lrs:\n",
    "    accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=memo_tta, augmentation=RandomResizedCrop, n_aug=8, adam=True, lr=lr)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_lr = lr\n",
    "\n",
    "lr_adam = best_lr\n",
    "offset = lr_adam / 2\n",
    "\n",
    "lrs = [lr_adam - offset, lr_adam, lr_adam + offset]\n",
    "for lr in lrs:\n",
    "    accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=memo_tta, augmentation=RandomResizedCrop, n_aug=8, adam=True, lr=lr)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_lr = lr\n",
    "\n",
    "lr_adam = best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T12:32:25.847510Z",
     "start_time": "2024-07-17T12:28:47.703193Z"
    }
   },
   "outputs": [],
   "source": [
    "# implementation of: test time adaptation via MEMO, without single sample batch norm\n",
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=memo_tta, n_aug=8, adam=True, lr=lr_adam)\n",
    "\n",
    "print(f\"top-1 accuracy for memo: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:17:36.248007Z",
     "start_time": "2024-07-16T20:17:24.058773Z"
    }
   },
   "outputs": [],
   "source": [
    "# implementation of: test time adaptation via MEMO, with single sample batch norm\n",
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=memo_tta, BN_prior_strength=16, adam=True, lr=lr_adam)\n",
    "\n",
    "print(f\"top-1 accuracy for memo+single sample BN: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:17:44.132868Z",
     "start_time": "2024-07-16T20:17:36.248007Z"
    }
   },
   "outputs": [],
   "source": [
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=memo_tta, augmentation=random_hide_patch, adam=True, lr=lr_adam)\n",
    "\n",
    "print(f\"top-1 accuracy for Random Patch Hiding: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:17:50.742008Z",
     "start_time": "2024-07-16T20:17:44.132868Z"
    }
   },
   "outputs": [],
   "source": [
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"vit\", adaptation_method=memo_tta, augmentation=random_hide_patch,\n",
    "                                                 augmentation_selection=True, lr=lr_adam)\n",
    "\n",
    "print(f\"top-1 accuracy for Random Patch Hiding+augmentation selection: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"resnet\", adaptation_method=no_adaptation)\n",
    "\n",
    "print(f\"top-1 accuracy for pretrained model without TTA: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of: test time adaptation via MEMO, without single sample batch norm\n",
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"resnet\", adaptation_method=memo_tta)\n",
    "\n",
    "print(f\"top-1 accuracy for memo: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of: test time adaptation via MEMO, with single sample batch norm\n",
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"resnet\", adaptation_method=memo_tta, BN_prior_strength=16)\n",
    "\n",
    "print(f\"top-1 accuracy for memo+single sample BN: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"resnet\", adaptation_method=memo_tta, augmentation=random_hide_patch)\n",
    "\n",
    "print(f\"top-1 accuracy for Random Patch Hiding: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_accuracy = experiment_adaptation_on_dataset(model=\"resnet\", adaptation_method=memo_tta, augmentation=random_hide_patch,\n",
    "                                                 augmentation_selection=True)\n",
    "\n",
    "print(f\"top-1 accuracy for Random Patch Hiding+augmentation selection: {top1_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of results\n",
    "Since there is no training, adaptation is done in small epochs, and adaptation is done each time on fresh pretrained model, I could not present learning curves.\n",
    "Instead, the accuracy on dataset for different techniques is presented below:\n",
    "\n",
    "| Adaptation Technique                       | Top-1 Accuracy  |\n",
    "|--------------------------------------------|-----------------|\n",
    "| Baseline (ResNet50)                        | 0.026%          |\n",
    "| MEMO                                       | 0.013%          |\n",
    "| MEMO+BN                                    | **0.6% (best)** |\n",
    "| Random Patch Hiding                        | 0.026%          |\n",
    "| Random Patch Hiding+augmentation selection | 0.026%          |\n",
    "| MEMO (lr=0.00007, epochs_for_adaptation=2) | 0.026%          |\n",
    "\n",
    "\n",
    "| Adaptation Technique | Top-1 Accuracy |\n",
    "|----------------------|----------------|\n",
    "| Baseline (ResNet18)  | 1.1%           |\n",
    "| MEMO                 | 1.1%           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change in accuracy is insignificant. In the original paper, they managed to produce a 0.3% top-1 accuracy. The difference in results might be due to having multiple runs and reporting the best result.\n",
    "\n",
    "We also experimented with ResNet18 model. Interestingly, the accuracy is higher, although no improvement achieved with adaptation techniques. The better baseline accuracy of ResNet18 could be due to overfitting of ResNet50 model to spurious cues.\n",
    "\n",
    "Overall, the classifiers reliance on spurious cues resulting in poor generalization is shown by imagenet-a dataset. Although the dataset may not be a good benchmark for testing TTA, the results indicate the importance of BN adaptation for single sample TTA.\n",
    "\n",
    "In the original paper, they experimented with different hyperparameters and reported the best result:\n",
    "- Learning rate η: 10^−3, 10^−4, 10^−5, 10^−6; then, 5×, 2.5×, and 0.5× the best value.\n",
    "- Number of gradient steps: 1, 2.\n",
    "- % of the maximum loss value to threshold for adaptation: 50, 100.\n",
    "- Prior strength N : 8, 16, 32.\n",
    "- Number of augmentations: 32, 64.\n",
    "- Model: ResNet-50, ResNext-101s.\n",
    "\n",
    "A critique of original work is that they did not test the effect of their adaptation technique on normal ImageNet dataset. Does MEMO decrease the performance on normal ImageNet dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
